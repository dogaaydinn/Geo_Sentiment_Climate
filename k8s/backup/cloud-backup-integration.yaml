# Cloud Provider Backup Integration
# Automated backups to S3, GCS, and Azure Blob Storage
#
# Features:
# - Automated daily/hourly backups
# - Multi-cloud support
# - Lifecycle policies
# - Encryption at rest
# - Point-in-time recovery
# - Backup verification
#
# Installation:
#   kubectl apply -f k8s/backup/cloud-backup-integration.yaml

---
# ============================================================================
# AWS S3 Backup Configuration
# ============================================================================

# S3 Bucket Configuration (via Terraform or AWS CLI)
# Terraform example:
# resource "aws_s3_bucket" "geo_climate_backups" {
#   bucket = "geo-climate-backups-${var.environment}"
#
#   lifecycle_rule {
#     enabled = true
#
#     transition {
#       days          = 30
#       storage_class = "STANDARD_IA"
#     }
#
#     transition {
#       days          = 90
#       storage_class = "GLACIER"
#     }
#
#     expiration {
#       days = 365
#     }
#   }
#
#   versioning {
#     enabled = true
#   }
#
#   server_side_encryption_configuration {
#     rule {
#       apply_server_side_encryption_by_default {
#         sse_algorithm     = "aws:kms"
#         kms_master_key_id = aws_kms_key.backup.arn
#       }
#     }
#   }
# }

---
# AWS Backup CronJob for Database
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-s3-backup
  namespace: geo-climate
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 3

  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: postgres-backup
        spec:
          serviceAccountName: geo-climate-backup  # With S3 write permissions

          containers:
            - name: backup
              image: postgres:15-alpine

              command:
                - /bin/bash
                - -c
                - |
                  set -e

                  TIMESTAMP=$(date +%Y%m%d_%H%M%S)
                  BACKUP_FILE="geo_climate_${TIMESTAMP}.sql.gz"
                  S3_BUCKET="s3://geo-climate-backups-${ENVIRONMENT}/postgres/"

                  echo "Starting backup at ${TIMESTAMP}..."

                  # Create compressed backup with pg_dump
                  PGPASSWORD=$POSTGRES_PASSWORD pg_dump \
                    -h postgres \
                    -U $POSTGRES_USER \
                    -d $POSTGRES_DB \
                    --clean \
                    --if-exists \
                    --no-owner \
                    --no-privileges | \
                    gzip > /tmp/${BACKUP_FILE}

                  # Get file size
                  FILESIZE=$(stat -f%z "/tmp/${BACKUP_FILE}" 2>/dev/null || stat -c%s "/tmp/${BACKUP_FILE}")
                  echo "Backup file size: $(numfmt --to=iec-i --suffix=B $FILESIZE)"

                  # Upload to S3 with encryption
                  aws s3 cp /tmp/${BACKUP_FILE} ${S3_BUCKET}${BACKUP_FILE} \
                    --storage-class STANDARD_IA \
                    --server-side-encryption aws:kms \
                    --metadata "database=${POSTGRES_DB},timestamp=${TIMESTAMP}"

                  # Verify upload
                  aws s3 ls ${S3_BUCKET}${BACKUP_FILE}

                  # Cleanup
                  rm /tmp/${BACKUP_FILE}

                  # Test restore (sample)
                  echo "Running restore test on 10 rows..."
                  aws s3 cp ${S3_BUCKET}${BACKUP_FILE} - | \
                    gunzip | head -n 10 > /dev/null

                  echo "Backup completed successfully!"

                  # Cleanup old backups (keep last 30 days in STANDARD_IA)
                  aws s3 ls ${S3_BUCKET} | \
                    awk '{print $4}' | \
                    while read file; do
                      FILE_DATE=$(echo $file | grep -oP '\d{8}')
                      if [ ! -z "$FILE_DATE" ]; then
                        DAYS_OLD=$(( ( $(date +%s) - $(date -d $FILE_DATE +%s) ) / 86400 ))
                        if [ $DAYS_OLD -gt 30 ]; then
                          echo "File $file is $DAYS_OLD days old, moving to Glacier..."
                        fi
                      fi
                    done

              env:
                - name: POSTGRES_USER
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secrets
                      key: POSTGRES_USER
                - name: POSTGRES_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secrets
                      key: POSTGRES_PASSWORD
                - name: POSTGRES_DB
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secrets
                      key: POSTGRES_DB
                - name: ENVIRONMENT
                  value: "production"
                - name: AWS_DEFAULT_REGION
                  value: "us-east-1"

              resources:
                requests:
                  memory: "512Mi"
                  cpu: "250m"
                limits:
                  memory: "2Gi"
                  cpu: "1000m"

          restartPolicy: OnFailure

---
# ============================================================================
# Google Cloud Storage (GCS) Backup Configuration
# ============================================================================

# GCS Backup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-gcs-backup
  namespace: geo-climate
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 3

  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: postgres-backup
        spec:
          serviceAccountName: geo-climate-backup  # With GCS write permissions

          containers:
            - name: backup
              image: google/cloud-sdk:alpine

              command:
                - /bin/bash
                - -c
                - |
                  set -e

                  # Install PostgreSQL client
                  apk add --no-cache postgresql15-client

                  TIMESTAMP=$(date +%Y%m%d_%H%M%S)
                  BACKUP_FILE="geo_climate_${TIMESTAMP}.sql.gz"
                  GCS_BUCKET="gs://geo-climate-backups-${ENVIRONMENT}/postgres/"

                  echo "Starting backup at ${TIMESTAMP}..."

                  # Create backup
                  PGPASSWORD=$POSTGRES_PASSWORD pg_dump \
                    -h postgres \
                    -U $POSTGRES_USER \
                    -d $POSTGRES_DB \
                    --clean \
                    --if-exists | \
                    gzip > /tmp/${BACKUP_FILE}

                  # Upload to GCS with lifecycle policy
                  gsutil -o GSUtil:encryption_key=$GCS_ENCRYPTION_KEY \
                    cp /tmp/${BACKUP_FILE} ${GCS_BUCKET}${BACKUP_FILE}

                  # Set object metadata
                  gsutil setmeta \
                    -h "x-goog-meta-database:${POSTGRES_DB}" \
                    -h "x-goog-meta-timestamp:${TIMESTAMP}" \
                    ${GCS_BUCKET}${BACKUP_FILE}

                  # Verify upload
                  gsutil ls -l ${GCS_BUCKET}${BACKUP_FILE}

                  # Cleanup local file
                  rm /tmp/${BACKUP_FILE}

                  echo "Backup completed successfully!"

                  # Lifecycle policy (applied via gsutil or Terraform)
                  # gsutil lifecycle set lifecycle.json gs://geo-climate-backups

              env:
                - name: POSTGRES_USER
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secrets
                      key: POSTGRES_USER
                - name: POSTGRES_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secrets
                      key: POSTGRES_PASSWORD
                - name: POSTGRES_DB
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secrets
                      key: POSTGRES_DB
                - name: ENVIRONMENT
                  value: "production"
                - name: GCS_ENCRYPTION_KEY
                  valueFrom:
                    secretKeyRef:
                      name: gcs-encryption-key
                      key: key

              resources:
                requests:
                  memory: "512Mi"
                  cpu: "250m"
                limits:
                  memory: "2Gi"
                  cpu: "1000m"

          restartPolicy: OnFailure

---
# GCS Lifecycle Policy (lifecycle.json)
# {
#   "lifecycle": {
#     "rule": [
#       {
#         "action": {"type": "SetStorageClass", "storageClass": "NEARLINE"},
#         "condition": {"age": 30}
#       },
#       {
#         "action": {"type": "SetStorageClass", "storageClass": "COLDLINE"},
#         "condition": {"age": 90}
#       },
#       {
#         "action": {"type": "SetStorageClass", "storageClass": "ARCHIVE"},
#         "condition": {"age": 180}
#       },
#       {
#         "action": {"type": "Delete"},
#         "condition": {"age": 365}
#       }
#     ]
#   }
# }

---
# ============================================================================
# Azure Blob Storage Backup Configuration
# ============================================================================

# Azure Blob Backup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-azure-backup
  namespace: geo-climate
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 3

  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: postgres-backup
        spec:
          serviceAccountName: geo-climate-backup  # With Azure Storage permissions

          containers:
            - name: backup
              image: mcr.microsoft.com/azure-cli:latest

              command:
                - /bin/bash
                - -c
                - |
                  set -e

                  # Install PostgreSQL client
                  apt-get update && apt-get install -y postgresql-client-15

                  TIMESTAMP=$(date +%Y%m%d_%H%M%S)
                  BACKUP_FILE="geo_climate_${TIMESTAMP}.sql.gz"
                  CONTAINER="postgres-backups"
                  STORAGE_ACCOUNT="geoclimatebackups${ENVIRONMENT}"

                  echo "Starting backup at ${TIMESTAMP}..."

                  # Create backup
                  PGPASSWORD=$POSTGRES_PASSWORD pg_dump \
                    -h postgres \
                    -U $POSTGRES_USER \
                    -d $POSTGRES_DB \
                    --clean \
                    --if-exists | \
                    gzip > /tmp/${BACKUP_FILE}

                  # Upload to Azure Blob with encryption
                  az storage blob upload \
                    --account-name ${STORAGE_ACCOUNT} \
                    --container-name ${CONTAINER} \
                    --name ${BACKUP_FILE} \
                    --file /tmp/${BACKUP_FILE} \
                    --tier Cool \
                    --metadata database=${POSTGRES_DB} timestamp=${TIMESTAMP} \
                    --encryption-scope backup-encryption

                  # Verify upload
                  az storage blob show \
                    --account-name ${STORAGE_ACCOUNT} \
                    --container-name ${CONTAINER} \
                    --name ${BACKUP_FILE}

                  # Cleanup local file
                  rm /tmp/${BACKUP_FILE}

                  echo "Backup completed successfully!"

                  # Lifecycle management (set via Azure Portal or CLI)
                  # Move to Archive tier after 90 days
                  # Delete after 365 days

              env:
                - name: POSTGRES_USER
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secrets
                      key: POSTGRES_USER
                - name: POSTGRES_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secrets
                      key: POSTGRES_PASSWORD
                - name: POSTGRES_DB
                  valueFrom:
                    secretKeyRef:
                      name: postgres-secrets
                      key: POSTGRES_DB
                - name: ENVIRONMENT
                  value: "production"
                - name: AZURE_STORAGE_CONNECTION_STRING
                  valueFrom:
                    secretKeyRef:
                      name: azure-storage-secrets
                      key: connection-string

              resources:
                requests:
                  memory: "512Mi"
                  cpu: "250m"
                limits:
                  memory: "2Gi"
                  cpu: "1000m"

          restartPolicy: OnFailure

---
# ============================================================================
# Application State Backup (Models, Cache)
# ============================================================================

# ML Model Backup to S3
apiVersion: batch/v1
kind: CronJob
metadata:
  name: model-backup
  namespace: geo-climate
spec:
  schedule: "0 2 * * 0"  # Weekly on Sunday at 2 AM

  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: geo-climate-backup

          containers:
            - name: backup
              image: amazon/aws-cli:latest

              command:
                - /bin/bash
                - -c
                - |
                  set -e

                  TIMESTAMP=$(date +%Y%m%d)
                  S3_BUCKET="s3://geo-climate-backups-production/models/"

                  # Sync models directory to S3
                  aws s3 sync /models ${S3_BUCKET}${TIMESTAMP}/ \
                    --storage-class STANDARD_IA \
                    --metadata "backup-date=${TIMESTAMP}"

                  # Create tarball for point-in-time snapshot
                  tar -czf /tmp/models_${TIMESTAMP}.tar.gz /models
                  aws s3 cp /tmp/models_${TIMESTAMP}.tar.gz ${S3_BUCKET}snapshots/
                  rm /tmp/models_${TIMESTAMP}.tar.gz

                  echo "Model backup completed!"

              volumeMounts:
                - name: model-storage
                  mountPath: /models

              resources:
                requests:
                  memory: "256Mi"
                  cpu: "100m"
                limits:
                  memory: "1Gi"
                  cpu: "500m"

          volumes:
            - name: model-storage
              persistentVolumeClaim:
                claimName: model-storage-pvc

          restartPolicy: OnFailure

---
# ============================================================================
# Backup Verification CronJob
# ============================================================================

apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-verification
  namespace: geo-climate
spec:
  schedule: "0 4 * * *"  # Daily at 4 AM

  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: geo-climate-backup

          containers:
            - name: verify
              image: postgres:15-alpine

              command:
                - /bin/bash
                - -c
                - |
                  set -e

                  echo "Starting backup verification..."

                  # Get latest backup from S3
                  LATEST_BACKUP=$(aws s3 ls s3://geo-climate-backups-production/postgres/ | \
                    sort | tail -n 1 | awk '{print $4}')

                  echo "Latest backup: ${LATEST_BACKUP}"

                  # Download and test restore
                  aws s3 cp s3://geo-climate-backups-production/postgres/${LATEST_BACKUP} /tmp/test_backup.sql.gz

                  # Verify gzip integrity
                  gunzip -t /tmp/test_backup.sql.gz
                  echo "✓ Backup file integrity verified"

                  # Test SQL parsing (first 1000 lines)
                  gunzip -c /tmp/test_backup.sql.gz | head -n 1000 | psql --set ON_ERROR_STOP=on -v ON_ERROR_STOP=1 -d template1 > /dev/null
                  echo "✓ SQL syntax verified"

                  # Check backup age
                  BACKUP_DATE=$(echo ${LATEST_BACKUP} | grep -oP '\d{8}')
                  CURRENT_DATE=$(date +%Y%m%d)
                  DAYS_OLD=$(( ( $(date -d $CURRENT_DATE +%s) - $(date -d $BACKUP_DATE +%s) ) / 86400 ))

                  if [ $DAYS_OLD -gt 1 ]; then
                    echo "⚠ WARNING: Latest backup is ${DAYS_OLD} days old!"
                    exit 1
                  else
                    echo "✓ Backup is fresh (${DAYS_OLD} days old)"
                  fi

                  # Cleanup
                  rm /tmp/test_backup.sql.gz

                  echo "Backup verification completed successfully!"

              env:
                - name: AWS_DEFAULT_REGION
                  value: "us-east-1"

              resources:
                requests:
                  memory: "256Mi"
                  cpu: "100m"
                limits:
                  memory: "1Gi"
                  cpu: "500m"

          restartPolicy: OnFailure

---
# ============================================================================
# Backup Monitoring
# ============================================================================

# PrometheusRule for backup monitoring
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: backup-alerts
  namespace: geo-climate
spec:
  groups:
    - name: backup
      interval: 30s
      rules:
        - alert: BackupFailed
          expr: |
            kube_job_status_failed{job_name=~"postgres.*backup.*"} > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Backup job {{ $labels.job_name }} failed"
            description: "Backup job has failed. Check logs immediately."

        - alert: BackupTooOld
          expr: |
            time() - kube_job_status_completion_time{job_name=~"postgres.*backup.*"} > 86400
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "No recent backup found"
            description: "Last successful backup was more than 24 hours ago."

        - alert: BackupVerificationFailed
          expr: |
            kube_job_status_failed{job_name="backup-verification"} > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Backup verification failed"
            description: "Backup integrity check failed. Investigate immediately."

---
# ============================================================================
# BACKUP RESTORE PROCEDURES
# ============================================================================
#
# AWS S3 Restore:
# --------------
# 1. List available backups:
#    aws s3 ls s3://geo-climate-backups-production/postgres/ --region us-east-1
#
# 2. Download backup:
#    aws s3 cp s3://geo-climate-backups-production/postgres/geo_climate_20250115_020000.sql.gz /tmp/
#
# 3. Stop API:
#    kubectl scale deployment geo-climate-api --replicas=0 -n geo-climate
#
# 4. Restore database:
#    gunzip -c /tmp/geo_climate_20250115_020000.sql.gz | \
#      kubectl exec -i postgres-0 -n geo-climate -- \
#      psql -U geo_climate -d geo_climate_prod
#
# 5. Verify data:
#    kubectl exec -it postgres-0 -n geo-climate -- \
#      psql -U geo_climate -d geo_climate_prod -c "SELECT COUNT(*) FROM users;"
#
# 6. Restart API:
#    kubectl scale deployment geo-climate-api --replicas=3 -n geo-climate
#
#
# GCS Restore:
# -----------
# 1. List backups:
#    gsutil ls gs://geo-climate-backups-production/postgres/
#
# 2. Download:
#    gsutil cp gs://geo-climate-backups-production/postgres/geo_climate_20250115_020000.sql.gz /tmp/
#
# 3. Follow steps 3-6 from AWS restore
#
#
# Azure Blob Restore:
# ------------------
# 1. List backups:
#    az storage blob list \
#      --account-name geoclimatebackupsproduction \
#      --container-name postgres-backups
#
# 2. Download:
#    az storage blob download \
#      --account-name geoclimatebackupsproduction \
#      --container-name postgres-backups \
#      --name geo_climate_20250115_020000.sql.gz \
#      --file /tmp/backup.sql.gz
#
# 3. Follow steps 3-6 from AWS restore
#
# ============================================================================
# BACKUP STRATEGY SUMMARY
# ============================================================================
#
# Retention Policy:
# ----------------
# - Hourly: Keep for 24 hours
# - Daily: Keep for 30 days (STANDARD_IA/Cool tier)
# - Weekly: Keep for 90 days (Glacier/Archive tier)
# - Monthly: Keep for 1 year (Deep Archive)
#
# Backup Windows:
# --------------
# - Database: Every 6 hours
# - Models: Weekly (Sunday 2 AM)
# - Verification: Daily (4 AM)
#
# Storage Costs (estimated):
# -------------------------
# AWS S3:
#   - Standard (0-30 days): $0.023/GB/month
#   - Standard-IA (30-90 days): $0.0125/GB/month
#   - Glacier (90-365 days): $0.004/GB/month
#   - Deep Archive (>365 days): $0.00099/GB/month
#
# For 100GB database with 6-month retention:
#   - ~30 backups in Standard: $69/month
#   - ~60 backups in Standard-IA: $75/month
#   - ~90 backups in Glacier: $36/month
#   Total: ~$180/month
#
# ============================================================================
