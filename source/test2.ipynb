{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T13:25:14.904143Z",
     "start_time": "2025-01-10T13:25:14.840998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import sys\n",
    "from utils.logger import setup_logger\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from source.utils.config_loader import load_config\n"
   ],
   "id": "8512ed8191aaba9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T13:25:14.916355Z",
     "start_time": "2025-01-10T13:25:14.914364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Projeye özel local script import:\n",
    "sys.path.append(os.path.abspath(\"../source\"))"
   ],
   "id": "aa45f5d36c0c01ef",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T13:25:36.655938Z",
     "start_time": "2025-01-10T13:25:36.646515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CONFIG_PATH = \"../config/settings.yml\"\n",
    "\n",
    "try:\n",
    "    # Config dosyasını yükle\n",
    "    config = load_config(CONFIG_PATH)\n",
    "    print(config)\n",
    "\n",
    "    # Dizinleri config'den al\n",
    "    RAW_DIR = config[\"paths\"][\"raw_dir\"]\n",
    "    PROCESSED_DIR = config[\"paths\"][\"processed_dir\"]\n",
    "    PLOTS_DIR = config[\"paths\"].get(\"plots_dir\", \"../plots\")\n",
    "    LOG_DIR = config[\"paths\"].get(\"logs_dir\", \"../logs\")\n",
    "\n",
    "    # Dizinlerin varlığını kontrol et ve oluştur\n",
    "    os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "    # Logger oluştur\n",
    "    logger = setup_logger(name=\"data_preprocessing\", log_file=os.path.join(LOG_DIR, \"data_preprocessing.log\"),\n",
    "                          log_level=\"INFO\")\n",
    "    logger.info(\"Config dosyası ve dizinler başarıyla yüklendi.\")\n",
    "except KeyError as e:\n",
    "    raise ValueError(f\"Config dosyasındaki bir anahtar eksik: {e}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Config yüklenirken hata oluştu: {e}\")\n",
    "\n"
   ],
   "id": "741b2d8500098412",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 16:25:36,653 - data_preprocessing - INFO - Config dosyası ve dizinler başarıyla yüklendi.\n",
      "2025-01-10 16:25:36,653 - data_preprocessing - INFO - Config dosyası ve dizinler başarıyla yüklendi.\n",
      "INFO:data_preprocessing:Config dosyası ve dizinler başarıyla yüklendi.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project_name': 'Geo_Sentiment_Climate', 'paths': {'raw_dir': '../data/raw/', 'interim_dir': '../data/interim/', 'processed_dir': '../data/processed/', 'metadata_dir': '../data/metadata/', 'plots_dir': '../plots/', 'logs_dir': '../logs/', 'archive_dir': '../data/archive/'}, 'rename_map_common': {'Date': 'date', 'Units': 'units', 'Daily AQI Value': 'aqi', 'State FIPS Code': 'state_fips', 'County FIPS Code': 'county_fips'}, 'rename_map_param': {'so2': 'Daily Max 1-hour SO2 Concentration', 'o3': 'Daily Max 8-hour Ozone Concentration', 'co': 'Daily Max 8-hour CO Concentration', 'no2': 'Daily Max 1-hour NO2 Concentration', 'pm25': 'Daily Mean PM2.5 Concentration'}, 'data_check': {'required_columns': ['Date', 'Source', 'Site ID', 'POC', 'Daily Max 1-hour SO2 Concentration', 'Units', 'Daily AQI Value', 'Local Site Name', 'Daily Obs Count', 'Percent Complete', 'AQS Parameter Code', 'AQS Parameter Description', 'Method Code', 'CBSA Code', 'CBSA Name', 'State FIPS Code', 'State', 'County FIPS Code', 'County', 'Site Latitude', 'Site Longitude'], 'optional_columns': ['Source', 'Site ID', 'POC', 'Local Site Name', 'Daily Obs Count', 'Percent Complete', 'Method Code', 'CBSA Code', 'CBSA Name', 'State FIPS Code', 'County FIPS Code']}, 'parameters': {'missing_value_method': 'mean', 'scaling_method': 'standard', 'max_rows': 5000}, 'eda': {'numeric_threshold': 0.75, 'max_categories': 20}, 'logging': {'log_file': '../logs/project.log', 'log_level': 'INFO', 'log_format': '%(asctime)s - %(levelname)s - %(message)s'}}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T13:25:39.077433Z",
     "start_time": "2025-01-10T13:25:39.067714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(file_path):\n",
    "    logger.info(f\"Loading data from {file_path}\")\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        logger.info(\"Data loaded successfully\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_plot(plot, filename):\n",
    "    filepath = os.path.join(PLOTS_DIR, filename)\n",
    "    plot.savefig(filepath)\n",
    "    logger.info(f\"Plot saved to {filepath}\")\n",
    "    plt.close(plot)\n",
    "\n",
    "\n",
    "def basic_info(df):\n",
    "    logger.info(\"Generating basic info of the dataset\")\n",
    "    info = {\n",
    "        \"Shape\": df.shape,\n",
    "        \"Columns\": df.columns.tolist(),\n",
    "        \"Data Types\": df.dtypes.to_dict(),\n",
    "        \"Missing Values\": df.isnull().sum().to_dict()\n",
    "    }\n",
    "    logger.info(f\"Dataset Info: {info}\")\n",
    "    return info\n",
    "\n",
    "\n",
    "def visualize_missing_values(df, save=True):\n",
    "    logger.info(\"Visualizing missing values.\")\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        msno.matrix(df)\n",
    "        if save:\n",
    "            plot_path = os.path.join(PLOTS_DIR, \"missing_values_matrix.png\")\n",
    "            plt.savefig(plot_path)\n",
    "            logger.info(f\"Missing values matrix plot saved to {plot_path}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in missing values visualization: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def fill_missing_values(df, method=\"mean\"):\n",
    "    logger.info(f\"Filling missing values using method: {method}\")\n",
    "    try:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if method == \"mean\":\n",
    "            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "        elif method == \"median\":\n",
    "            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        elif method == \"mode\":\n",
    "            df = df.fillna(df.mode().iloc[0])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method for missing value handling\")\n",
    "        logger.info(\"Missing values filled successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error filling missing values: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def remove_null_values(df, subset_cols):\n",
    "    logger.info(\"Removing rows with null values\")\n",
    "    try:\n",
    "        df_clean = df.dropna(subset=subset_cols)\n",
    "        logger.info(\"Null values removed successfully\")\n",
    "        return df_clean\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in removing null values: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def distribution_analysis(df, numeric_cols):\n",
    "    logger.info(\"Performing distribution analysis for numeric columns\")\n",
    "    try:\n",
    "        for col in numeric_cols:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.histplot(df[col], kde=True, bins=30, color=\"blue\")\n",
    "            plt.title(f\"Distribution of {col}\")\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in distribution analysis: {e}\")\n",
    "\n",
    "\n",
    "def scale_features(df, numeric_cols, method=\"standard\"):\n",
    "    logger.info(f\"Scaling features: {numeric_cols} using method: {method}\")\n",
    "    try:\n",
    "        scaler = StandardScaler() if method == \"standard\" else MinMaxScaler()\n",
    "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "        logger.info(\"Features scaled successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scaling features: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def correlation_analysis(df, numeric_cols):\n",
    "    logger.info(\"Performing correlation analysis.\")\n",
    "    try:\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "        plot_path = os.path.join(PLOTS_DIR, \"correlation_matrix.png\")\n",
    "        plt.title(\"Correlation Matrix\")\n",
    "        plt.savefig(plot_path)\n",
    "        logger.info(f\"Correlation matrix plot saved to {plot_path}\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in correlation analysis: {e}\")\n",
    "\n",
    "\n",
    "def detect_outliers(df, numeric_cols, save=True):\n",
    "    logger.info(\"Detecting outliers in numeric columns.\")\n",
    "    try:\n",
    "        for col in numeric_cols:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.boxplot(x=df[col])\n",
    "            plt.title(f\"Outliers in {col}\")\n",
    "            if save:\n",
    "                save_plot(plt, f\"outliers_{col}.png\")\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in outlier detection: {e}\")\n"
   ],
   "id": "12f8d353ce141fe",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T13:25:40.435117Z",
     "start_time": "2025-01-10T13:25:39.991309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = load_config(\"../config/settings.yml\")\n",
    "\n",
    "file_path = os.path.join(PROCESSED_DIR, \"epa_long_preprocessed.csv\")\n",
    "\n",
    "try:\n",
    "    df = load_data(file_path)\n",
    "    logger.info(\"Data loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Failed to load data: {e}\")\n",
    "    raise\n",
    "\n",
    "file_path = \"data/processed/epa_long_preprocessed.csv\""
   ],
   "id": "87f1ffd01c18c8ab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 16:25:39,994 - data_preprocessing - INFO - Loading data from ../data/processed/epa_long_preprocessed.csv\n",
      "2025-01-10 16:25:39,994 - data_preprocessing - INFO - Loading data from ../data/processed/epa_long_preprocessed.csv\n",
      "INFO:data_preprocessing:Loading data from ../data/processed/epa_long_preprocessed.csv\n",
      "2025-01-10 16:25:40,000 - data_preprocessing - ERROR - Error loading data: [Errno 2] No such file or directory: '../data/processed/epa_long_preprocessed.csv'\n",
      "2025-01-10 16:25:40,000 - data_preprocessing - ERROR - Error loading data: [Errno 2] No such file or directory: '../data/processed/epa_long_preprocessed.csv'\n",
      "ERROR:data_preprocessing:Error loading data: [Errno 2] No such file or directory: '../data/processed/epa_long_preprocessed.csv'\n",
      "2025-01-10 16:25:40,002 - data_preprocessing - CRITICAL - Failed to load data: [Errno 2] No such file or directory: '../data/processed/epa_long_preprocessed.csv'\n",
      "2025-01-10 16:25:40,002 - data_preprocessing - CRITICAL - Failed to load data: [Errno 2] No such file or directory: '../data/processed/epa_long_preprocessed.csv'\n",
      "CRITICAL:data_preprocessing:Failed to load data: [Errno 2] No such file or directory: '../data/processed/epa_long_preprocessed.csv'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed/epa_long_preprocessed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(PROCESSED_DIR, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepa_long_preprocessed.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m----> 6\u001B[0m     df \u001B[38;5;241m=\u001B[39m load_data(file_path)\n\u001B[1;32m      7\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData loaded successfully\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m, in \u001B[0;36mload_data\u001B[0;34m(file_path)\u001B[0m\n\u001B[1;32m      2\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading data from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m----> 4\u001B[0m     data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(file_path)\n\u001B[1;32m      5\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData loaded successfully\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/miniconda3/envs/geoenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[0;32m~/miniconda3/envs/geoenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/miniconda3/envs/geoenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[0;32m~/miniconda3/envs/geoenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[1;32m   1881\u001B[0m     f,\n\u001B[1;32m   1882\u001B[0m     mode,\n\u001B[1;32m   1883\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1884\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1885\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m   1886\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[1;32m   1887\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1888\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1889\u001B[0m )\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/miniconda3/envs/geoenv/lib/python3.12/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    874\u001B[0m             handle,\n\u001B[1;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m    876\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[1;32m    877\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[1;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    879\u001B[0m         )\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/processed/epa_long_preprocessed.csv'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1123c82e2d3197f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "939fc79247f2b4d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T13:25:40.593992Z",
     "start_time": "2025-01-10T13:25:40.591524Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d4e47f9c3b938dd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T13:25:40.828124Z",
     "start_time": "2025-01-10T13:25:40.825780Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d9fba57ec12f2905",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e209a8964d258db5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
